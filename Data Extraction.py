# -*- coding: utf-8 -*-
"""Extracting Text from Blog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1evGgAftEzNvEDtW2TaQej-dc5fgKklRO

## Using Beautiful Soup for Data Extraction and saving the Title and content of the blog in a .txt file with URL_ID as the file name
"""
print("Download the assignment folder as shared into a folder named BLACKCOFFER")
print("COmmand Line Arguments:")
input1 = input("1. Provide: Input File containing URL_ID & URL ")
input2 = input("2. Provide: Folder name to store the output files obtained from web scraping in .txt format ")
input3 = input("3. Provide:  Folder name containing stopwords files ")
input4 = input("4. Provide: Create an empty folder to store the filtered text(without stopwords) back to the file ")

import requests
import pandas as pd
from bs4 import BeautifulSoup
import sys
import os

def scrape_blogs(df):
    page_not_found = []
    for i in range(0,len(df)):
        URL_ID,URL = df.iloc[i,0],df.iloc[i,1]
        print(URL)
        response = requests.get(URL)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            entry_title = soup.find('h1', class_='entry-title')
            if entry_title==None:
                entry_title = soup.find('h1', class_='tdb-title-text')

            content = soup.find('div',class_="td-post-content tagdiv-type")
            if content==None:
                content = soup.find('div',class_="td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type")
                if content==None:
                    content = soup.find('div',class_="td-post-content tagdiv-type")

            file_content = entry_title.text+"\n"+content.text
            decoded_content = ""
            words = file_content.split()  # Split the content into words
            
            for word in words:
                try:
                    decoded_word = word.encode('utf-8').decode('utf-8')
                    decoded_content += decoded_word + " "
                except UnicodeEncodeError:
                    # Ignore words that cannot be encoded using UTF-8
                    pass

            with open(f'{input2}/{URL_ID}.txt', 'w', encoding="utf-8") as f:
                f.write(decoded_content)


            # with open(f'{input2}/{URL_ID}.txt' , 'w',encoding="utf-8") as f:
            #     f.write(file_content)
            print(f"{URL_ID} Done")
        else:
            page_not_found.append(URL_ID)
            print("Failed to retrieve page.")



# #Scraping the blogs
df = pd.read_excel(input1)
scrape_blogs(df)
# -*- coding: utf-8 -*-
"""Text Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dqm-T8fuK75AXP22cXrInMOVhZ__nSHD
"""

import nltk
nltk.download('punkt')
nltk.download('cmudict')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
import pandas as pd
import regex as re
import sys

# from nltk.tokenize import word_tokenize
def read_text_from_file(file_path):
    text = ""
    with open(file_path, 'r') as file:
        for line in file:
            try:
                text += line
            except UnicodeDecodeError:
                # Ignore the line if it cannot be decoded using UTF-8
                pass
    return text

def read_text_from_file_2(file_path):
    text = ""
    with open(file_path, 'r',encoding="utf-8") as file:
        for line in file:
            try:
                text += line
            except UnicodeDecodeError:
                # Ignore the line if it cannot be decoded using UTF-8
                pass
    return text

def remove_stopwords(text, stopwords_list):
    # Tokenize the text into words
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stopwords_list]
    filtered_text = ' '.join(filtered_words)
    return filtered_text

import string


# Path to the directory containing text files
text_files_dir = input2

# Path to the directory containing stopwords files
stopwords_dir = input3

stopwords_list = []
for file_name in os.listdir(stopwords_dir):
    stopwords_file_path = os.path.join(stopwords_dir, file_name)
    stopwords_list.extend(read_text_from_file(stopwords_file_path).split())

stopwords_list = list(set(stopwords_list))
for i,stopword in enumerate(stopwords_list):
  stopwords_list[i] = stopword.lower()

for file_name in os.listdir(text_files_dir):
    text_file_path = os.path.join(text_files_dir, file_name)
    text = read_text_from_file_2(text_file_path)
    filtered_text = remove_stopwords(text, stopwords_list)

    # Save the filtered text back to the file
    filtered_text_file_path = os.path.join(f'{input4}', f'{file_name}.txt')
    with open(filtered_text_file_path, 'w', encoding='utf-8') as file:
        file.write(filtered_text)
    print(f"Done {file_name}.txt")

print("Stopwords removed and filtered text files saved successfully.")

def read_words(file_path):
    words = set()
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        for line in file:
            try:
                word = line.strip()
                words.add(word)
            except UnicodeDecodeError:
                continue
    return words

positive_words = read_words("positive-words.txt")
negative_words = read_words("negative-words.txt")

# print(positive_words)
# print(negative_words)

from collections import Counter

def calculate_positive_score(text, positive_words):
    # Tokenize the text into words
    words = text.split()

    # Calculate positive score
    positive_score = sum(1 for word in words if word.lower() in positive_words)

    return positive_score

# Function to calculate negative score for text
def calculate_negative_score(text, negative_words):
    # Tokenize the text into words
    words = text.split()

    # Calculate negative score
    negative_score = sum(-1 for word in words if word.lower() in negative_words)

    return negative_score
import string

# Function to calculate total words in cleaned text
def calculate_total_words(text):
    text_no_punct = text.translate(str.maketrans('', '', string.punctuation))
    words = text_no_punct.split()
    total_words = len(words)

    return total_words

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import cmudict

# Function to calculate the number of syllables in a word
def syllable_count(word):
    d = cmudict.dict()
    if word.lower() in d:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])
    else:
        # Estimate the number of syllables based on the number of vowels
        return max(1, sum(word.count(c) for c in 'aeiou'))

# Function to calculate the number of complex words in the text
def count_complex_words(x):

    syllable = 'aeiou'

    t = x.split()

    v = []

    for i in t:
        words = i.split()
        c=Counter()

        for word in words:
            c.update(set(word))

        n = 0
        for a in c.most_common():
            if a[0] in syllable:
                if a[1] >= 2:
                    n += 1

        m = 0
        p = []
        for a in c.most_common():
            if a[0] in syllable:
                p.append(a[0])
        if len(p) >= 2:
            m += 1

        if n >= 1 or m >= 1:
            v.append(i)

    return len(v)

# Function to calculate Average Sentence Length
def calculate_average_sentence_length(text):
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    num_sentences = len(sentences)
    num_words = len(words)
    if num_sentences > 0:
        return num_words / num_sentences
    else:
        return 0

# Function to calculate Percentage of Complex words
def calculate_percentage_complex_words(text):
    words = word_tokenize(text)
    num_complex_words = count_complex_words(text)
    num_total_words = len(words)
    if num_total_words > 0:
        return (num_complex_words / num_total_words) * 100
    else:
        return 0

# Function to calculate Fog Index
def calculate_fog_index(text):
    avg_sentence_length = calculate_average_sentence_length(text)
    percentage_complex_words = calculate_percentage_complex_words(text)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)
    return fog_index

def calculate_total_sentences(text):
    sentences = sent_tokenize(text)
    return len(sentences)

# Function to count syllables in a word
def count_syllables(word):
    vowels = 'aeiouy'
    num_syllables = 0
    prev_char_was_vowel = False

    for char in word:
        if char.lower() in vowels:
            # If the previous character was not a vowel, increase the syllable count
            if not prev_char_was_vowel:
                num_syllables += 1
            prev_char_was_vowel = True
        else:
            prev_char_was_vowel = False

    # Exception handling for words ending with "es" or "ed"
    if word.endswith(('es', 'ed')) and len(word) > 2:
        num_syllables -= 1

    # Ensure that at least one syllable is counted for each word
    return max(1, num_syllables)


  # Function to count personal pronouns in text
def count_personal_pronouns(text):
    personal_pronouns = ["I", "we", "my", "ours", "us"]
    pattern = re.compile(r'\b(?:' + '|'.join(personal_pronouns) + r')\b', re.IGNORECASE)
    matches = pattern.findall(text)
    matches = [match for match in matches if match.lower() != 'us']
    count = len(matches)

    return count

def calculate_average_word_length(text):
    words = text.split()
    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]
    total_characters = sum(len(word) for word in words)
    total_words = len(words)
    if total_words > 0:
        average_word_length = total_characters / total_words
    else:
        average_word_length = 0

    return average_word_length

from tqdm import tqdm

df = pd.read_excel('Output Data Structure.xlsx')
for index,i in tqdm(enumerate(df["URL_ID"].values)):

  file_name = os.path.join(f"{input4}",i+".txt.txt")
  try:
    text = ""
    with open(file_name, 'r', encoding='utf-8') as file:
        for line in file:
            for word in line:
                try:
                    text += word
                except UnicodeDecodeError:
                    # Ignore the line if it cannot be decoded using UTF-8
                    pass
            text+="\n"
            

    total_words = text.split(' ')
    pos_score = calculate_positive_score(text,positive_words)
    neg_score = -1*calculate_negative_score(text,negative_words)
    total_words = calculate_total_words(text)
    complex_words = count_complex_words(text)
    average_sentence_length = calculate_average_sentence_length(text)
    total_sentences = calculate_total_sentences(text)

    df.loc[index,"POSITIVE SCORE"] = pos_score
    df.loc[index,"NEGATIVE SCORE"] = neg_score
    df.loc[index,"POLARITY SCORE"] = (pos_score-neg_score)/((pos_score+neg_score) + 0.000001)
    df.loc[index,"SUBJECTIVITY SCORE"] = (pos_score + neg_score)/ ((total_words) + 0.000001)
    df.loc[index,"AVG SENTENCE LENGTH"] = average_sentence_length
    df.loc[index,"PERCENTAGE OF COMPLEX WORDS"] = (complex_words / total_words)*100
    df.loc[index,"FOG INDEX"] = 0.4 * (average_sentence_length +  (complex_words / total_words)*100)
    df.loc[index,"AVG NUMBER OF WORDS PER SENTENCE"] = total_words/total_sentences
    df.loc[index,"COMPLEX WORD COUNT"] = complex_words
    df.loc[index,"WORD COUNT"] = total_words
    df.loc[index,"SYLLABLE PER WORD"] = count_syllables(text)
    df.loc[index,"PERSONAL PRONOUNS"] = count_personal_pronouns(text)
    df.loc[index,"AVG WORD LENGTH"] = calculate_average_word_length(text)
  except Exception as e:
      print(index,e)
      df.loc[index,"POSITIVE SCORE"] = "Error 404"
      df.loc[index,"NEGATIVE SCORE"] = "Error 404"
      df.loc[index,"POLARITY SCORE"] = "Error 404"
      df.loc[index,"SUBJECTIVITY SCORE"] = "Error 404"
      df.loc[index,"AVG SENTENCE LENGTH"] = "Error 404"
      df.loc[index,"PERCENTAGE OF COMPLEX WORDS"] = "Error 404"
      df.loc[index,"FOG INDEX"] = "Error 404"
      df.loc[index,"AVG NUMBER OF WORDS PER SENTENCE"] = "Error 404"
      df.loc[index,"COMPLEX WORD COUNT"] = "Error 404"
      df.loc[index,"WORD COUNT"] = "Error 404"
      df.loc[index,"SYLLABLE PER WORD"] = "Error 404"
      df.loc[index,"PERSONAL PRONOUNS"] = "Error 404"
      df.loc[index,"AVG WORD LENGTH"] = "Error 404"
  print("Done ",index)


df.to_csv("Output.csv")

